

# Active fire segmentation with deep learning: a transfer learning study from Landsat-8 to Sentinel-2

To segment active fire, we explored the bands similarities between Landsat-8 and Sentinel-2 to train a U-net with images of the former and adjust it using a single image to the latter, as depicted in the image below:

<img src="abstract.png" width="600">

We use the dataset built by [Pereira et al. (2021)]() to train Convolutional Neural Networks (CNN) to segmentate active fire using Landsat-8 images, then we explored different approachs for Transfer Learning to adapt the networks for Sentinel-2 images. To do that, we explore bands with similar wavelenghts in these satellites. Despite the similarities, the bands are not strictly equivalent, therefore, there are some differences in the information captured by the satellites. To overcome these differents we used a Batch Normalization layer to adjust the Sentinel-2 values.  

This repository contains instructions to download the images used in the expiriments and the weights for both the networks trained with Landsat-8 images and the models fine-tunned with Sentinel-2 images.

## Authors

[Andre Minoro Fusioka](https://github.com/Minoro)

[Gabriel Henrique de Almeida Pereira](https://github.com/pereira-gha)

[Bogdan Tomoyuki Nassu](https://github.com/btnassu)

[Rodrigo Minetto](https://github.com/rminetto)


# Datasets

We used two datasets to perform this work. The first consist of Landsat-8 images, with masks generated by thresholding algorithms. The second with Sentinel-2 images manually annotated. To train the base network, we used the Landsat-8 dataset built in the work [Active fire detection in Landsat-8 imagery: A large-scale dataset and a deep-learning study](https://www.sciencedirect.com/science/article/abs/pii/S092427162100160X), this dataset in available on [GitHub/Google Drive](https://github.com/pereira-gha/activefire/). 

With the base networks ready, we used a single Sentinel-2 image to adjust to the new satellite. The second dataset consist of 26 Sentinel-2 images, visually inspected and manually annotated. From the 26 images, 22 contains at least one fire pixel, while 4 images has no fire. The disposition of the images around the world can be seen in the picture below.

<img src="map_active_fire_annotations_image_id.png" width="600">

The Sentinel-2 images and the manual annotations are available in the [TODO](), we provide the entire scene with 5490x5490 pixels and also the patches with 256x256 pixels (12,584 patches in total). The images (and patches) have all 20m bands, but in this expirement we used only three. In addition to the manual annotation we also provide the masks generetad by [Murphy et al. (2016)](), [Liu et al. (2022)]() and [Kato and Nakamura, (2022)]() methods, and the masks generated by combinations of these methods, using their intersection and a voting scheme (at least two methods must agree where fires occur).

# Training the base network

We trained five different U-net using Landsat-8 images and masks. To train these networks we used the data available in the repository built by [Pereira et al.](https://github.com/pereira-gha/activefire/). Although the authors provided the weights of the trained models, we chose to use different bands, so it was necessary to train the models with the appropriate bands. We trained the models using the bands SWIR-2 (band-7), SWIR-1 (band-6) and NIR (band-5) from Landsat-8 and the masks created by the [Kumar and Roy (2018)](), [Murphy et al. (2016)](),  [Schroeder et al. (2016)](), and their combination by intersection and majority-voting.

If you want to train the base model by yourself you can follow the instructions available in the [Pereira et al.](https://github.com/pereira-gha/activefire/) repository, but it is necessary to adjust the bands used to train the models. In this repository we provide the trained weights from the networks trained with the mentioned dataset, you can follow the instructions in the [Downloading the pre-trained weights for Landsat-8](#downloading-the-pre-trained-weights-for-landsat-8). 

## Downloading the pre-trained weights for Landsat-8

Besides the datasets and code, you can download the weights for the trained models. They are available on [TODO](). We provide the weights for the U-net trained with Landsat-8 images and the models after the fine-tunning with Sentinel-2 images.

For Landsat-8, the network was trained with 256x256-pixels patches, using the bands SWIR-2 (band-7), SWIR-1 (band-6) and NIR (band-5) forming a 3-channel image. The masks used to train the model was created by the [Kumar and Roy (2018)](), [Murphy et al. (2016)](),  [Schroeder et al. (2016)](), and their combination by intersection and majority-voting. Each of these set of masks was used to train an indepentend base model, therefore, there are 5 base model available trained with Lansat-8 data. The table below shows the directory in this repository for each base model:

| Satellite | Masks                    | Trained Model Path                                                                                         |
|-----------|--------------------------|------------------------------------------------------------------------------------------------------------|
| Landsat-8 | Kumar and Roy (2018)     | `resources/landsat/weights/kumar-roy/unet_64f_2conv_765/model_unet_Kumar-Roy_765_final_weights.h5`         |
| Landsat-8 | Murphy et al. (2016)     | `resources/landsat/weights/murphy/unet_64f_2conv_765/model_unet_Murphy_765_final_weights.h5`               |
| Landsat-8 | Schroeder et al. (2016)  | `resources/landsat/weights/schroeder/unet_64f_2conv_765/model_unet_Schroeder_765_final_weights.h5`         |
| Landsat-8 | Intersection             | `resources/landsat/weights/intersection/unet_64f_2conv_765/model_unet_Intersection_765_final_weights.h5`   |
| Landsat-8 | Voting                   | `resources/landsat/weights/voting/unet_64f_2conv_765/model_unet_Voting_765_final_weights.h5`               |

# Fine-tune with Sentinel-2 images

This section will guide you in how to fine-tune the trained models to Sentinel-2, if you don't want to fine-tune by yourself you can download the final models as described in the section [Downloading the Sentinel-2 fine-tuned models](#downloading-the-sentinel-2-fine-tuned-models). 

After training the base networks with Landsat-8 images, we performed transfer-learning using similar bands of Sentinel-2. In our experiments we reserved the five images with the most fire patches to form five distinct folds to perform the fine-tuning to Sentinel-2 (the other images was used to evalute the model), each of the five images was used separately as a "fold", repeting the expiriments five times (one for each train image).


The first step to fine-tuning is to define which images are used for training, validation and testing. In this repository there is the script `src/transfer-learning/generate_kfolds_manual_annotation.py` that can be used to perform this division. The first time you run this script it will process all annotations computing the number of fire pixels and the maximum value of the bands used and saved to the file defined in `CSV_WITH_NUMBER_OF_FIRE_PIXELS_OF_EACH_PATCH` constant, if the file already exists it will not be re-computed. You can change the images directory in the `IMAGES_DIR` constant and the annotations directory in the `MASKS_DIR` constant. If you don't want to use five images/folds you can change the value in the `NUM_TOP_FIRE_IMAGES_TO_TRAIN` constant. The proportion of fire-patches reserved to validation can be set in the `VALIDATION_FRACTION` constant, the default value is 20%. The constant named `NON_FIRE_PROPORTION` defined the proportion of patches without fire to be used to fine-tuning the networks, the default value is 1, this means that for each fire-patch one patch without fire will be used. The constants `OUTPUT_PATH` and `OUTPUT_FILE` define the directory and the file of the output. Once you have your script configured you can run:

```shell
python generate_kfolds_manual_annotation.py
```

The file generated by this script contains the patches to be used for each fold of the experiments, dividing the images into their respective set (train/validation/test), this file will be used to fine-tuning the base model to Sentinel-2 images.

It is important to keep in mind that, despite the bands from Lansat-8 and Sentinel-2 shares similarities they are not strictly equivalent, also, the spatial resolution is different for each satellite (30m to Lansat-8 and 20m to Sentinel-2), therefore the network needs to learn how to understand this changes. In order adapt the network we added an batch normalization layer immediately after the input for the Sentinel-2 images.

The script `scr/transfer-learning/architecture_64f.py` can be used to fine-tune the base networks, it needs the folds files (generated before) to be set in the constant `FOLDS_FILE`. It will fine-tune each base model, if you want to use a specific base model you can change the list in the `PRE_TRAINED_MASKS` constant, the names defined in this list refers to the (authors) name of the method used generated masks used to train the base model. You need to set the constant `PRE_TRAINED_WEIGHTS_DIR` to the path were the weights of the base model are stored. We evaluated three transfer learning strategy, you can control those you want to use in the list `CONFIG`, the value `freeze_all` will freeze all weights learned from Landsat-8 images, the `unfreeze` will keep the weights unfrozen and free to learn, and `freeze_encoder` will freeze the weights of the first half of tto fine-tuning is to define which images are used for training, validation and testing. In this repository there is the script `src/transfer-learning/generate_kfolds_manual_annotation.py` that can be used to perform this division. The first time you run this script it will process all annotations computing the number of fire pixels and the maximum value of the bands used and saved to the file defined in `CSV_WITH_NUMBER_OF_FIRE_PIXELS_OF_EACH_PATCH` constant, if the file already exists it will not be re-computed. You can change the images directory in the `IMAGES_DIR` constant and the annotations directory in the `MASKS_DIR` constant. If you don't want to use five images/folds you can change the value in the `NUM_TOP_FIRE_IMAGES_TO_TRAIN` constant. The proportion of fire-patches reserved to validation can be set in the `VALIDATION_FRACTION` constant, the default value is 20%. The constant named `NON_FIRE_PROPORTION` defined the proportion of patches without fire to be used to fine-tuning the networks, the default value is 1, this means that for each fire-patch one patch without fire will be used. The constants `OUTPUT_PATH` and `OUTPUT_FILE` define the directory and the file of the output. Once you have your script configured you can run:


```shell
python generate_kfolds_manual_annotation.py
```

The file generated by this script contains the patches to be used for each fold of the experiments, dividing the images into their respective set (train/validation/test), this file will be used to fine-tuning the base model to Sentinel-2 images.

It is important to keep in mind that, despite the bands from Lansat-8 and Sentinel-2 shares similarities they are not strictly equivalent, also, the spatial resolution is different for each satellite (30m to Lansat-8 and 20m to Sentinel-2), therefore the network needs to learn how to understand this changes. In order adapt the network we added an batch normalization layer immediately after the input for the Sentinel-2 images.

The script `src/transfer-learning/architecture_64f.py` can be used to fine-tune the base networks, it needs the folds files (generated before) to be set in the constant `FOLDS_FILE`. It will fine-tune each base model, if you want to use a specific base model you can change the list in the `PRE_TRAINED_MASKS` constant, the names defined in this list refers to the (authors) name of the method used generated masks used to train the base model. You need to set the constant `PRE_TRAINED_WEIGHTS_DIR` to the path were the weights of the base model are stored. We evaluated three transfer learning strategy, you can control those you want to use in the list `CONFIG`, the value `freeze_all` will freeze all weights learned from Landsat-8 images, the `unfreeze` will keep the weights unfrozen and free to learn, and `freeze_encoder` will freeze the weights of the first half of the U-net (Encoder). If you change the path of the dataset with patches and manual annotations you need to change the `IMAGES_DIR` and `MASKS_DIR` to the correct directories. Also, if you are not using the default five folds you need to change the `FOLDS` constant. You can run the script with the command: 

```shell
python architecture_64f.py
```

If you don't want to use the initial Batch Normalization layer you can remove it from the code, or just change the constant `NORMALIZATION_MODE` to `no-bn`. If you set the value to `None` you need to adjust image scale properly to use the pre-trained networks.

This script will fine-tune the networks and evaluated them, each model will be saved the the `WEIGHTS_DIR` directory, and the results will be saved as json files in the `OUTPUT_DIR` directory. It will be save the train history, the results over the test set and the fine-tuned model. The final model can be used to make predictions over new images. 

The mentioned script will perform the fine-tuning using the manual annotations, we also provide the script `src/transfer-learning/architecture_64f_method.py` to fine-tune the models using masks generated by thresholding algorithms. It is very similar to the previous script, but you need to set `METHOD` constant to the threholding method that will be used to fine-tune the networks and the `MASKS_DIR` to the path with the 256x256-pixels masks generated by the method.


# Downloading the Sentinel-2 fine-tuned models

We provide the models trained after the fine-tune with Sentinel-2 image. While the network trained with the Landsat-8 is provided as weights (.h5), the fine-tuned models are provided as Keras models and can be easly be loaded from the disk.

We evaluated 100 models with manual annotations: for each base-model (5), we evaluated 5-folds using 3 transfer learning strategy, and one with randomly initialized weights. Therefore we have 5 x 5 x (3 + 1) models, but due to size limitations we provide only the best weights of each transfer learning strategy. If you want to use the models with the default configuration of the scripts of this repository you need to put the downloaded models at the directory:

| TL Strategy       | Base Model (Trained with L8 data)  | Fine-Tuning Masks     | Model Path                                                                                                                                 |
|-------------------|------------------------------------|-----------------------|---------------------------------------------------------------------------------------------------------------------|
| Frozen            | Intersection                       | Manual Annotation     | `resources/transfer_learning/output/weights/5fold_8020_mask1_unet64f_bce_augmentation/Intersection_fold_3_nsamples_freeze_all_bce`         |
| Frozen-Encoder    | Intersection                       | Manual Annotation     | `resources/transfer_learning/output/weights/5fold_8020_mask1_unet64f_bce_augmentation/Intersection_fold_4_nsamples_freeze_encoder_bce`     |
| Unfrozen          | Voting                             | Manual Annotation     | `resources/transfer_learning/output/weights/5fold_8020_mask1_unet64f_bce_augmentation/Voting_fold_2_nsamples_unfreeze_bce`                 |


If you trained the models by yourself the models will be available at same directory as shown in the table above, under the `resources/transfer_learning/output/weights/5fold_8020_mask1_unet64f_bce_augmentation` folder, where `5fold_8020_mask1_unet64f_bce_augmentation` is the name of the model (define in the `MODEL_NAME` constant of the `src/transfer-learning/architecture_64f.py` script).

You may notice that the Landsat-8 weights are available as a `h5` file while the fine-tuned models are available as Keras models. This is because the fine-tuned model have an extra layer at the beginning of the model. Using keras you can load the model with `tf.keras.models.load_model`, an example of how to load the models is available in the `src/transfer-learning/best_models_inference.py` script.


# Generating active fire masks for Sentinel-2

The fine-tuned models can generate be used to segmentate Sentinel-2 images and generate the active fire masks. Besides the trained networks you can also generate the active fire masks using the tresholding algorithms available in this repository. We provide three methods that can be used in Sentinel-2 images, namely [Kato-Nakamura](), [Liu et al.]() and [Murphy et al.](). Note that, the Murphy et al. method was originally developed for Landsat-8, but use bands with similarities with Sentinel-2. All methods rely on bands SWIR-2 (band-12), SWIR-1 (bands-11) and NIR (band-8A) from Sentinel-2.

If you want to generate active fire masks for Landsat-8, you can check the repository built by [Pereira et al.](https://github.com/pereira-gha/activefire/).

## Generating active fire masks with deep learning

With the fine-tuned model you can use it to generate active fire masks for Sentinel-2. If you used the code to fine-tune from scrach it will generated the predictions and evaluate the models, but it not save the predictions. You can use the script `best_models_inference.py` to generate the predictions using the best model from the folds. It needs the results of each fold to evaluate which one perform better, the best model will be loaded and will be used to generate the active fire masks.

In the script, if needed, you can change the `FOLDS_FILE` to the file with the definition of the train/test/validation folds. If you changed the default model name you need to change the `MODELS` constant. The `MODELS_PATH` constant must be set to the path of the fine-tuned models and `RESULTS_MODELS_PATH` to the directory were the results of the transfer learning was saved. You can change the images and masks path using the constants `IMAGES_DIR` and `MASKS_DIR`.

Inside the `src/transfer-learning` directory you can run:

```shell
python best_models_inference.py
```

The predictions will be saved inside the `OUTPUT_DIR` directory.


## Generating active fire masks with threshold methods

The thresholding methods requires the entire scenes (5490x5490 pixels) and the metadata to generate the active fire masks. The implementation of the thresholding methods can be found in the `src/active_fire/general.py`, this script is used by `src/thresholding/generate_afi_mask.py` to process the dataset images and generate the active fire. You can define where the Sentinel-2 images in the `IMAGES_STACK_DIR` constant and the metadata in `METADATA_DIR` constant. The images must be a stack of 20m bands with the postfix `_20m_stack.tif`. You can choose the methods you want to use in the constant named `ALGORITHMS`, each method will generate a mask for each image found. If you want to process only some images, you can define the prefix of the stacks in the `STACKS` constant. Inside the `src/thresholding` you can run:

```shell
python generate_afi_mask.py
```

By default it will save the masks as TIF images in the folder defined in the `OUTPUT_DIR` constant, each method will create a folder and the masks will be saved inside. Note that the masks have 5490x5490 pixels, if you want to use they to fine-tune the network you need to crop the masks as 256x256-pixels. If you set the constant `SAVE_AS_TXT` to `True` the prediction will be saved as a txt file, this file can be used to evaluate the performance of the methods.

## Evaluating the thresholding algorithms

After generating the algorithms predictions you can evalute the performance of the methods compared with the manual annotations. The script `src/transfer-learning/evaluate_fold_afi_algorithms.py` will evaluate the output produced by the methods and save the results to the disk. You need to set `PREDICTION_FORMAT` to the format of the images (accepted values: txt or tif). The `PREDICTIONS_DIR` should be set to the directory of the predictions produced by the thresholding algorithms, the `ANNOTATIONS_DIR` must be the path of the directory of the manual annotations, the predictions and annotations must be in the same format. The `DATAFRAME_FOLDS_PATH` define the csv with the definition of the used folds, with train/validation/test division. The output will be saved in json at `RESULTS_OUTPUT_DIR` directory. Inside the `src/transfer-learning` you can run:

```shell
python evaluate_fold_afi_algorithms.py
```

# Utils scripts

We provide some scripts with utils funcionalities, for example, if you want to combine the thresholding masks by intersection or voting using the script `src/utils/make_voting_and_intersection_masks.py`. You need to set the path for the original images (5490x5490-pixels) in the `IMAGES_STACK_PATH` constant and `MASKS_PATH` to the masks path of the thresholding algoriths. The outputs will be saved at `OUTPUT_PATH` directory. You can run it inside the `src/utils/` folder:

```shell
python make_voting_and_intersection_masks.py
```

The script to make the predictions of the thresholding algoriths allows you to save the output as txt file, these files can be used to evaluate the algorithms. If you chose this format, you will need annotations in the same format, to convert the annotations to this format you can run (inside the utils folder):

```shell
python annotations_to_txt.py
```

We provide a script to rebuild the annotations from the 256x256-pixels patches. It read the 20m image stack to retrive the meta information about the location of the image, then it will read each the 256x256-pixels patches and reorganise them by the geo-location, generating the final image. You need to set the `STACKS_PATH` constant to the path with the Sentinel-2 images with the 20m bands. The `CROPED_ANNOTATIONS_PATH` constant must be set to the path with the annotations in 256x256 format. The final image will be saved at `OUTPUT_SCENE_ANNOATIONS_PATH`. To use the script you can run:  

```shell
python rebuild_annotations_scene.py
```

Similarly, you can rebuild the predictions produced by the networks using:

```shell
python rebuild_predictions_scene.py
```

You need to set the `CROPED_IMAGES_PATH` constant to the path with path were the predictions produced by the networks are stored.

Besides the utils script we also provide a Jupyter Notebook to help you see the results generated by the networks and algorithms, the notebook is `src/notebook/`


# Citation

TODO


