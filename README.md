

# Active fire segmentation with deep learning: a transfer learning study from Landsat-8 to Sentinel-2

To segment active fire, we explored the bands similarities between Landsat-8 and Sentinel-2 to train a U-net with images of the former and adjust it using a single image to the latter, as depicted in the image below:

<img src="abstract.png" width="600" height="275">

We use the dataset built by [Pereira et al. (2021)]() to train Convolutional Neural Networks (CNN) to segmentate active fire using Landsat-8 images, then we explored different approachs for Transfer Learning to adapt the networks for Sentinel-2 images. To do that, we explore bands with similar wavelenghts in these satellites. Despite the similarities, the bands are not strictly equivalent, therefore, there are some differences in the information captured by the satellites. To overcome these differents we used a Batch Normalization layer to adjust the Sentinel-2 values.  

This repository contains instructions to download the images used in the expiriments and the weights for both the networks trained with Landsat-8 images and the models fine-tunned with Sentinel-2 images.

## Authors

[Andre Minoro Fusioka](https://github.com/Minoro)

[Gabriel Henrique de Almeida Pereira](https://github.com/pereira-gha)

[Bogdan Tomoyuki Nassu](https://github.com/btnassu)

[Rodrigo Minetto](https://github.com/rminetto)


# Datasets

We used two datasets to perform this work. The first consist of Landsat-8 images, with masks generated by thresholding algorithms. The second with Sentinel-2 images manually annotated. To train the base network, we used the Landsat-8 dataset built in the work [Active fire detection in Landsat-8 imagery: A large-scale dataset and a deep-learning study](https://www.sciencedirect.com/science/article/abs/pii/S092427162100160X), this dataset in available on [GitHub/Google Drive](https://github.com/pereira-gha/activefire/). 

With the base networks ready, we used a single Sentinel-2 image to adjust to the new satellite. The second dataset consist of 26 Sentinel-2 images, visually inspected and manually annotated. From the 26 images, 22 contains at least one fire pixel, while 4 images has no fire. The disposition of the images around the world can be seen in the picture below.

<img src="map_active_fire_annotations_image_id.png" width="600" height="275">

The Sentinel-2 images and the manual annotations are available in the [TODO](), we provide the entinre scene with 5490x5490 pixels and also the patches with 256x256 pixels (12,584 patches in total). The patches have all 20m bands, but in this expirement we used only three. In addition to the manual annotation we also provide the masks generetad by [Murphy et al. (2016)](), [Liu et al. (2022)]() and [Kato and Nakamura, (2022)]() methods, and the masks generated by combinations of these methods, using their intersection and a voting scheme (at least two methods must agree where fires occur).

# Training the base network

We trained five different U-net using Landsat-8 images and masks. To train the base networks we used the data available in the repository built by [Pereira et al.](https://github.com/pereira-gha/activefire/). Although the authors provided the weights of the trained models, we chose to use different bands, so it was necessary to train the models with the appropriate bands. We trained the models using the bands SWIR-2 (band-7), SWIR-1 (band-6) and NIR (band-5) from Landsat-8 and the masks created by the [Kumar and Roy (2018)](), [Murphy et al. (2016)](),  [Schroeder et al. (2016)](), and their combination by intersection and majority-voting.

If you want to train the base model by yourself you can follow the instructions available in the [Pereira et al.](https://github.com/pereira-gha/activefire/) repository, but it is necessary to adjust the bands used to train the models. In this repository we provide the trained weights from the networks trained with the mentioned dataset, you can follow the instructions in the [Downloading the pre-trained weights for Landsat-8](#downloading-the-pre-trained-weights-for-landsat-8). 


# Fine-tune with Sentinel-2 images

This section will guide you in how to fine-tune the trained models to Sentinel-2, if you don't want to fine-tune by yourself you can download the final models as described in the section [Downloading the Sentinel-2 fine-tuned models](#downloading-the-sentinel-2-fine-tuned-models). 

After training the base networks with Landsat-8 images, we performed transfer-learning using similar bands of Sentinel-2. In our experiments we reserved the five images with the most fire patches to form five distinct folds to perform the fine-tuning to Sentinel-2 (the other images was used to evalute the model), each of the five images was used separately as a "fold", repeting the expiriments five times (one for each train image).


The first step to fine-tuning is to define which images are used for training, validation and testing. In this repository there is the script `src/transfer-learning/generate_kfolds_manual_annotation.py` that can be used to perform this division. The first time you run this script it will process all annotations computing the number of fire pixels and the maximum value of the bands used and saved to the file defined in `CSV_WITH_NUMBER_OF_FIRE_PIXELS_OF_EACH_PATCH` constant, if the file already exists it will not be re-computed. You can change the images directory in the `IMAGES_DIR` constant and the annotations directory in the `MASKS_DIR` constant. If you don't want to use five images/folds you can change the value in the `NUM_TOP_FIRE_IMAGES_TO_TRAIN` constant. The proportion of fire-patches reserved to validation can be set in the `VALIDATION_FRACTION` constant, the default value is 20%. The constant named `NON_FIRE_PROPORTION` defined the proportion of patches without fire to be used to fine-tuning the networks, the default value is 1, this means that for each fire-patch one patch without fire will be used. The constants `OUTPUT_PATH` and `OUTPUT_FILE` define the directory and the file of the output. Once you have your script configured you can run:

```shell
python generate_kfolds_manual_annotation.py
```

The file generated by this script contains the patches to be used for each fold of the experiments, dividing the images into their respective set (train/validation/test), this file will be used to fine-tuning the base model to Sentinel-2 images.

It is important to keep in mind that, despite the bands from Lansat-8 and Sentinel-2 shares similarities they are not strictly equivalent, also, the spatial resolution is different for each satellite (30m to Lansat-8 and 20m to Sentinel-2), therefore the network needs to learn how to understand this changes. In order adapt the network we added an batch normalization layer immediately after the input for the Sentinel-2 images.

The script `scr/transfer-learning/architecture_64f.py` can be used to fine-tune the base networks, it needs the folds files (generated before) to be set in the constant `FOLDS_FILE`. It will fine-tune each base model, if you want to use a specific base model you can change the list in the `PRE_TRAINED_MASKS` constant, the names defined in this list refers to the (authors) name of the method used generated masks used to train the base model. You need to set the constant `PRE_TRAINED_WEIGHTS_DIR` to the path were the weights of the base model are stored. We evaluated three transfer learning strategy, you can control those you want to use in the list `CONFIG`, the value `freeze_all` will freeze all weights learned from Landsat-8 images, the `unfreeze` will keep the weights unfrozen and free to learn, and `freeze_encoder` will freeze the weights of the first half of tto fine-tuning is to define which images are used for training, validation and testing. In this repository there is the script `src/transfer-learning/generate_kfolds_manual_annotation.py` that can be used to perform this division. The first time you run this script it will process all annotations computing the number of fire pixels and the maximum value of the bands used and saved to the file defined in `CSV_WITH_NUMBER_OF_FIRE_PIXELS_OF_EACH_PATCH` constant, if the file already exists it will not be re-computed. You can change the images directory in the `IMAGES_DIR` constant and the annotations directory in the `MASKS_DIR` constant. If you don't want to use five images/folds you can change the value in the `NUM_TOP_FIRE_IMAGES_TO_TRAIN` constant. The proportion of fire-patches reserved to validation can be set in the `VALIDATION_FRACTION` constant, the default value is 20%. The constant named `NON_FIRE_PROPORTION` defined the proportion of patches without fire to be used to fine-tuning the networks, the default value is 1, this means that for each fire-patch one patch without fire will be used. The constants `OUTPUT_PATH` and `OUTPUT_FILE` define the directory and the file of the output. Once you have your script configured you can run:


```shell
python generate_kfolds_manual_annotation.py
```

The file generated by this script contains the patches to be used for each fold of the experiments, dividing the images into their respective set (train/validation/test), this file will be used to fine-tuning the base model to Sentinel-2 images.

It is important to keep in mind that, despite the bands from Lansat-8 and Sentinel-2 shares similarities they are not strictly equivalent, also, the spatial resolution is different for each satellite (30m to Lansat-8 and 20m to Sentinel-2), therefore the network needs to learn how to understand this changes. In order adapt the network we added an batch normalization layer immediately after the input for the Sentinel-2 images.

The script `scr/transfer-learning/architecture_64f.py` can be used to fine-tune the base networks, it needs the folds files (generated before) to be set in the constant `FOLDS_FILE`. It will fine-tune each base model, if you want to use a specific base model you can change the list in the `PRE_TRAINED_MASKS` constant, the names defined in this list refers to the (authors) name of the method used generated masks used to train the base model. You need to set the constant `PRE_TRAINED_WEIGHTS_DIR` to the path were the weights of the base model are stored. We evaluated three transfer learning strategy, you can control those you want to use in the list `CONFIG`, the value `freeze_all` will freeze all weights learned from Landsat-8 images, the `unfreeze` will keep the weights unfrozen and free to learn, and `freeze_encoder` will freeze the weights of the first half of the U-net (Encoder). If you change the path of the dataset with patches and manual annotations you need to change the `IMAGES_DIR` and `MASKS_DIR` to the correct directories. Also, if you are not using the default five folds you need to change the `FOLDS` constant. You can run the script with the command: 

```shell
python architecture_64f.py
```

If you don't want to use the initial Batch Normalization layer you can remove it from the code, or just change the constant `NORMALIZATION_MODE` to `None`. If you set the value to `None` you need to adjust image scale properly to use the pre-trained networks.

This script will fine-tune the networks and evaluated them, each model will be saved the the `WEIGHTS_DIR` directory, and the results will be saved as json files in the `OUTPUT_DIR` directory. It will be save the train history, the results over the test set and the fine-tuned model. The final model can be used to make predictions over new images.


# Downloading the pre-trained weights for Landsat-8

Besides the datasets and code, you can download the weights for the trained models. They are available on [TODO](). We provide the weights for the U-net trained with Landsat-8 images and the models after the fine-tunning with Sentinel-2 images.

For Landsat-8, the network was trained with 256x256-pixels patches, using the bands SWIR-2 (band-7), SWIR-1 (band-6) and NIR (band-5) forming a 3-channel image. The masks used to train the model was created by the [Kumar and Roy (2018)](), [Murphy et al. (2016)](),  [Schroeder et al. (2016)](), and their combination by intersection and majority-voting. Each of these set of masks was used to train an indepentend base model, therefore, there are 5 base model available trained with Lansat-8 data. The table below shows the directory in this repository for each base model:

| Satellite | Masks                    | Trained Model Path                                                                                         |
|-----------|--------------------------|------------------------------------------------------------------------------------------------------------|
| Landsat-8 | Kumar and Roy (2018)     | `resources/landsat/weights/kumar-roy/unet_64f_2conv_765/model_unet_Kumar-Roy_765_final_weights.h5`         |
| Landsat-8 | Murphy et al. (2016)     | `resources/landsat/weights/murphy/unet_64f_2conv_765/model_unet_Murphy_765_final_weights.h5`               |
| Landsat-8 | Schroeder et al. (2016)  | `resources/landsat/weights/schroeder/unet_64f_2conv_765/model_unet_Schroeder_765_final_weights.h5`         |
| Landsat-8 | Intersection             | `resources/landsat/weights/intersection/unet_64f_2conv_765/model_unet_Intersection_765_final_weights.h5`   |
| Landsat-8 | Voting                   | `resources/landsat/weights/voting/unet_64f_2conv_765/model_unet_Voting_765_final_weights.h5`               |

# Downloading the Sentinel-2 fine-tuned models

TODO - Download the fine-tuned models

